#!/usr/bin/env python
"""
PII Scanner using AWS Comprehend

This tool scans files for personally identifiable information (PII) and other sensitive data
using AWS Comprehend and custom regex patterns. It can scan either all files in a directory
recursively or only files that have changed in a git repository.

Usage:
    ./pii_scan [options]

Options:
    --all                   Scan all files recursively (default: only scan git changes)
    --help, -h              Show this help message and exit
    --config FILE           Path to configuration file
    --min-confidence FLOAT  Minimum confidence score (0.0-1.0) for PII detection
    --output FORMAT         Output format: text, json, or csv (default: text)
    --custom-regex FILE     Path to file containing custom regex patterns
    --exclude-dirs DIRS     Comma-separated list of directories to exclude
    --exclude-exts EXTS     Comma-separated list of file extensions to exclude
    --workers INT           Number of worker threads (default: 8)
    --verbose, -v           Enable verbose output
    --quiet, -q             Suppress all output except findings and errors
    --region REGION         AWS region to use for Comprehend API
"""

import argparse
import boto3
from botocore.exceptions import NoRegionError, NoCredentialsError, ClientError
import concurrent.futures
import json
import os
import re
import string
import subprocess
import sys
import time
from collections import defaultdict
from typing import Dict, List, Tuple, Optional, Any, Set

# Default configuration
DEFAULT_CONFIG = {
    "max_workers": 8,  # Good default for github actions
    "max_bytes": 99000,  # AWS Comprehend's limit is 100KB per document
    "max_file_size": 10_000_000,  # 10MB max file size
    "text_check_blocksize": 512,  # Bytes to read for text file detection
    "context_window": 100,  # Characters of context to show around findings
    "batch_size": 25,  # AWS Comprehend batch API limit
    "max_retries": 3,  # Maximum number of retry attempts for AWS API calls
    "retry_delay": 1.0,  # Initial retry delay in seconds
    "min_confidence_score": 0.85,
    "excluded_dirs": {".git", ".venv", "node_modules", "__pycache__"},
    "excluded_extensions": {".min.js", ".map", ".svg", ".woff", ".ttf"},
    "critical_entity_types": {"AWS_ACCESS_KEY", "AWS_SECRET_KEY", "PASSWORD"},
    "security_relevant_entity_types": {
        "AWS_ACCESS_KEY",
        "AWS_SECRET_KEY",
        "PASSWORD",
    },
    "validation_patterns": {
        "AWS_ACCESS_KEY": r'^AKIA[0-9A-Z]{16}$',
        "AWS_SECRET_KEY": r'^[A-Za-z0-9/+=]{40}$',
        # Add more validation patterns as needed
    },
    "regex_patterns": {
        "Generic password assignment": r'(?i)(password\s*=\s*[\'"][^\'"]+[\'"])',
        "AWS Secret Key": r'(?i)(aws_secret_access_key\s*=\s*[\'"]?[A-Za-z0-9/+=]{40}[\'"]?)',
        "AWS Access Key": r'(?i)(aws_access_key_id\s*=\s*[\'"]?AKIA[0-9A-Z]{16}[\'"]?)',
        "Generic secret token": r'(?i)(secret(_key)?\s*=\s*[\'"][^\'"]+[\'"])',
        "Bearer token": r'(?i)(Bearer\s+[A-Za-z0-9\-_=]+\.[A-Za-z0-9\-_=]+\.[A-Za-z0-9\-_.+/=]*)'
    }
}


class PIIScanner:
    """Main class for PII scanning functionality"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the PII scanner with the given configuration
        
        Args:
            config: Dictionary containing configuration parameters
        """
        self.config = config
        self.aws_region = config.get("region")
        self.verbose = config.get("verbose", False)
        self.quiet = config.get("quiet", False)
        self.output_format = config.get("output_format", "text")
        self.full_scan_mode = config.get("full_scan", False)
        self.scan_path = config.get("scan_path", ".")
        
        # Validate configuration
        self._validate_config()
        
        # Compile regex patterns once for better performance
        self._compiled_patterns = {}
        self._compile_regex_patterns()
        
        # Initialize AWS Comprehend client
        session_kwargs = {}
        if self.aws_region:
            session_kwargs["region_name"] = self.aws_region
        
        try:
            self.comprehend = boto3.client("comprehend", **session_kwargs)
        except NoRegionError:
            print("\n" + "="*70, file=sys.stderr)
            print("ERROR: AWS Region Not Configured", file=sys.stderr)
            print("="*70, file=sys.stderr)
            print("\nYou must specify an AWS region to use AWS Comprehend.", file=sys.stderr)
            print("\nThere are several ways to fix this:\n", file=sys.stderr)
            print("1. Use the --region command line option:", file=sys.stderr)
            print("   ./pii_scan --region us-east-1\n", file=sys.stderr)
            print("2. Set the AWS_DEFAULT_REGION environment variable:", file=sys.stderr)
            print("   export AWS_DEFAULT_REGION=us-east-1\n", file=sys.stderr)
            print("3. Configure AWS CLI (recommended):", file=sys.stderr)
            print("   a. Install AWS CLI if not already installed:", file=sys.stderr)
            print("      - macOS: brew install awscli", file=sys.stderr)
            print("      - Linux: pip install awscli", file=sys.stderr)
            print("      - Windows: Download from https://aws.amazon.com/cli/\n", file=sys.stderr)
            print("   b. Configure AWS CLI with your credentials and region:", file=sys.stderr)
            print("      aws configure", file=sys.stderr)
            print("      (You'll be prompted for Access Key, Secret Key, region, and output format)\n", file=sys.stderr)
            print("   c. Alternatively, set just the region:", file=sys.stderr)
            print("      aws configure set region us-east-1\n", file=sys.stderr)
            print("Available regions for AWS Comprehend include:", file=sys.stderr)
            print("  us-east-1, us-east-2, us-west-2, eu-west-1, eu-central-1,", file=sys.stderr)
            print("  ap-southeast-1, ap-southeast-2, ap-northeast-1, ca-central-1", file=sys.stderr)
            print("\nFor more information, visit:", file=sys.stderr)
            print("  https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html", file=sys.stderr)
            print("="*70 + "\n", file=sys.stderr)
            sys.exit(1)
        except NoCredentialsError:
            print("\n" + "="*70, file=sys.stderr)
            print("ERROR: AWS Credentials Not Found", file=sys.stderr)
            print("="*70, file=sys.stderr)
            print("\nYou must configure AWS credentials to use AWS Comprehend.", file=sys.stderr)
            print("\nTo configure AWS credentials:\n", file=sys.stderr)
            print("1. Install AWS CLI if not already installed:", file=sys.stderr)
            print("   - macOS: brew install awscli", file=sys.stderr)
            print("   - Linux: pip install awscli", file=sys.stderr)
            print("   - Windows: Download from https://aws.amazon.com/cli/\n", file=sys.stderr)
            print("2. Run AWS configure:", file=sys.stderr)
            print("   aws configure", file=sys.stderr)
            print("   (You'll need your AWS Access Key ID and Secret Access Key)\n", file=sys.stderr)
            print("3. Alternatively, set environment variables:", file=sys.stderr)
            print("   export AWS_ACCESS_KEY_ID=your_access_key", file=sys.stderr)
            print("   export AWS_SECRET_ACCESS_KEY=your_secret_key", file=sys.stderr)
            print("   export AWS_DEFAULT_REGION=us-east-1\n", file=sys.stderr)
            print("For more information, visit:", file=sys.stderr)
            print("  https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html", file=sys.stderr)
            print("="*70 + "\n", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"\nERROR: Failed to initialize AWS Comprehend client: {e}", file=sys.stderr)
            print("Please check your AWS configuration and try again.\n", file=sys.stderr)
            sys.exit(1)
        
        # Load custom regex patterns if provided
        if "custom_regex_file" in config and config["custom_regex_file"]:
            self._load_custom_regex(config["custom_regex_file"])
            # Recompile patterns after loading custom ones
            self._compile_regex_patterns()
    
    def _validate_config(self) -> None:
        """Validate configuration parameters"""
        # Validate confidence score
        min_conf = self.config.get("min_confidence_score", 0.85)
        if not 0.0 <= min_conf <= 1.0:
            self._log_error(f"Invalid min_confidence_score: {min_conf}. Must be between 0.0 and 1.0")
            sys.exit(1)
        
        # Validate max_workers
        max_workers = self.config.get("max_workers", 8)
        if max_workers < 1:
            self._log_error(f"Invalid max_workers: {max_workers}. Must be at least 1")
            sys.exit(1)
        
        # Validate batch_size
        batch_size = self.config.get("batch_size", 25)
        if not 1 <= batch_size <= 25:
            self._log_error(f"Invalid batch_size: {batch_size}. Must be between 1 and 25")
            sys.exit(1)
    
    def _compile_regex_patterns(self) -> None:
        """Compile regex patterns for better performance"""
        self._compiled_patterns = {}
        for label, pattern in self.config["regex_patterns"].items():
            try:
                self._compiled_patterns[label] = re.compile(pattern)
            except re.error as e:
                self._log_error(f"Invalid regex pattern for '{label}': {e}")
                # Skip invalid patterns
                continue
        
        if self._compiled_patterns:
            self._log_verbose(f"Compiled {len(self._compiled_patterns)} regex patterns")
    
    def _load_custom_regex(self, file_path: str) -> None:
        """
        Load custom regex patterns from a file
        
        Args:
            file_path: Path to the file containing custom regex patterns
        """
        try:
            with open(file_path, 'r') as f:
                custom_patterns = json.load(f)
                
            if not isinstance(custom_patterns, dict):
                self._log_error(f"Custom regex file must contain a JSON object")
                return
                
            # Validate each pattern before adding
            valid_patterns = {}
            for label, pattern in custom_patterns.items():
                try:
                    re.compile(pattern)
                    valid_patterns[label] = pattern
                except re.error as e:
                    self._log_warning(f"Skipping invalid regex pattern '{label}': {e}")
            
            # Merge with existing patterns, overwriting any duplicates
            self.config["regex_patterns"].update(valid_patterns)
            self._log_verbose(f"Loaded {len(valid_patterns)} custom regex patterns")
        except json.JSONDecodeError:
            self._log_error(f"Failed to parse custom regex file: invalid JSON")
        except Exception as e:
            self._log_error(f"Failed to load custom regex file: {e}")
    
    def _log_verbose(self, message: str) -> None:
        """Log a message if verbose mode is enabled"""
        if self.verbose and not self.quiet:
            print(f"[INFO] {message}")
    
    def _log_error(self, message: str) -> None:
        """Log an error message"""
        if not self.quiet:
            print(f"[ERROR] {message}", file=sys.stderr)
    
    def _log_warning(self, message: str) -> None:
        """Log a warning message"""
        if not self.quiet:
            print(f"[WARNING] {message}", file=sys.stderr)
    
    def _log_progress(self, message: str, end: str = "\r") -> None:
        """Log a progress message"""
        if not self.quiet and not self.verbose:
            print(message, end=end)
    
    def is_text_file(self, path: str, blocksize: int = 512) -> bool:
        """
        Check if a file is a text file
        
        Args:
            path: Path to the file
            blocksize: Number of bytes to read for the check
            
        Returns:
            True if the file is a text file, False otherwise
        """
        # Skip certain file types known to cause false positives
        _, ext = os.path.splitext(path.lower())
        if ext in self.config["excluded_extensions"]:
            return False
        
        # Also check for combined extensions like .min.js
        if any(path.lower().endswith(ext) for ext in self.config["excluded_extensions"]):
            return False
            
        try:
            with open(path, 'rb') as f:
                chunk = f.read(blocksize)
                if not chunk:
                    return False
                text_chars = bytes(string.printable, 'utf-8')
                non_text = chunk.translate(None, text_chars)
                return float(len(non_text)) / len(chunk) < 0.30
        except Exception as e:
            self._log_verbose(f"Error checking if file is text: {path}: {e}")
            return False
    
    def split_text_to_chunks(self, text: str) -> List[str]:
        """
        Split text into chunks that are smaller than AWS Comprehend's limit.
        Uses a more aggressive approach for large files.
        
        Args:
            text: Text to split
            
        Returns:
            List of text chunks
        """
        chunks = []
        max_bytes = self.config["max_bytes"]
        
        # Check if entire text is small enough
        if len(text.encode('utf-8')) <= max_bytes:
            return [text]
        
        # For larger files, need to split by lines first
        lines = text.splitlines(True)  # keepends=True
        current_chunk = ""
        current_size = 0
        
        for line in lines:
            line_bytes = line.encode('utf-8')
            line_size = len(line_bytes)
            
            # Handle individual lines that are too large
            if line_size > max_bytes:
                # If we have accumulated text, add it as a chunk first
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = ""
                    current_size = 0
                
                # Split the large line into smaller chunks
                line_chunks = []
                for i in range(0, line_size, max_bytes):
                    # Get a chunk of bytes that's under the limit
                    chunk_bytes = line_bytes[i:i+max_bytes]
                    try:
                        # Convert bytes back to string, handling potential decoding errors
                        chunk_text = chunk_bytes.decode('utf-8', errors='replace')
                        line_chunks.append(chunk_text)
                    except UnicodeDecodeError:
                        # Skip if we can't decode
                        continue
                
                # Add all the line chunks
                chunks.extend(line_chunks)
            
            # Normal case: line is small enough to potentially add to current chunk
            elif current_size + line_size <= max_bytes:
                current_chunk += line
                current_size += line_size
            
            # Line would make current chunk too big, start a new one
            else:
                chunks.append(current_chunk)
                current_chunk = line
                current_size = line_size
        
        # Don't forget the last chunk
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def is_valid_entity(self, entity_type: str, entity_text: str) -> bool:
        """
        Validate if the detected entity matches expected patterns
        
        Args:
            entity_type: Type of the entity
            entity_text: Text of the entity
            
        Returns:
            True if the entity is valid, False otherwise
        """
        # If we don't have a validation pattern, accept it
        if entity_type not in self.config["validation_patterns"]:
            return True
        
        pattern = self.config["validation_patterns"][entity_type]
        return re.match(pattern, entity_text) is not None
    
    def detect_pii(self, text_chunk: str, file_text: str, offset: int = 0) -> List[Dict[str, Any]]:
        """
        Detect PII entities and enrich them with surrounding context
        
        Args:
            text_chunk: Text chunk to analyze
            file_text: Full file text (for context extraction)
            offset: Offset of the chunk in the full text
            
        Returns:
            List of detected PII entities
        """
        try:
            # Double-check size before sending to AWS
            chunk_size = len(text_chunk.encode('utf-8'))
            if chunk_size > self.config["max_bytes"]:
                self._log_warning(f"Chunk size {chunk_size} exceeds limit of {self.config['max_bytes']}. Skipping.")
                return []
            
            # Retry logic with exponential backoff for AWS API calls
            max_retries = self.config.get("max_retries", 3)
            retry_delay = self.config.get("retry_delay", 1.0)
            
            for attempt in range(max_retries):
                try:
                    response = self.comprehend.detect_pii_entities(Text=text_chunk, LanguageCode="en")
                    entities = response.get("Entities", [])
                    break
                except ClientError as e:
                    error_code = e.response.get('Error', {}).get('Code', '')
                    if error_code in ('ThrottlingException', 'TooManyRequestsException', 'ProvisionedThroughputExceededException'):
                        if attempt < max_retries - 1:
                            wait_time = retry_delay * (2 ** attempt)
                            self._log_verbose(f"AWS API throttled. Retrying in {wait_time:.1f}s (attempt {attempt + 1}/{max_retries})")
                            time.sleep(wait_time)
                        else:
                            self._log_error(f"AWS API throttled after {max_retries} attempts")
                            return []
                    else:
                        # Non-throttling error, don't retry
                        raise
            else:
                # Should not reach here, but handle gracefully
                return []
            
            validated_entities = []
            
            for entity in entities:
                # Apply minimum confidence threshold
                if entity["Score"] < self.config["min_confidence_score"]:
                    continue
                    
                # Extract the actual text of the entity
                start = entity["BeginOffset"]
                end = entity["EndOffset"]
                entity_text = text_chunk[start:end]
                
                # Skip entities that contain certain patterns indicating false positives
                if ',' in entity_text or ']' in entity_text or '[' in entity_text:
                    continue
                    
                # Validate entity against known patterns
                if entity["Type"] in self.config["validation_patterns"] and not self.is_valid_entity(entity["Type"], entity_text):
                    continue
                    
                # Store the entity text for reporting
                entity["EntityText"] = entity_text
                    
                # For critical entities, try to extract context
                if entity["Type"] in self.config["critical_entity_types"]:
                    # Adjust for the actual position in the chunk
                    actual_start = start + offset
                    actual_end = end + offset
                    
                    # Try to extract surrounding context (line or part of line)
                    try:
                        # Get a window around the entity
                        context_start = max(0, actual_start - 20)
                        context_end = min(len(file_text), actual_end + 20)
                        context = file_text[context_start:context_end].strip()
                        # Truncate if too long
                        if len(context) > 100:
                            context = context[:97] + '...'
                        entity["Context"] = context
                    except Exception as e:
                        self._log_verbose(f"Error extracting context: {e}")
                        
                validated_entities.append(entity)
                    
            return validated_entities
        except Exception as e:
            self._log_error(f"Error detecting PII: {e}")
            return []
    
    def scan_with_regex(self, text: str) -> List[Dict[str, Any]]:
        """
        Scan text with compiled regex patterns
        
        Args:
            text: Text to scan
            
        Returns:
            List of regex findings
        """
        findings = []
        for label, compiled_pattern in self._compiled_patterns.items():
            for match in compiled_pattern.finditer(text):
                findings.append({
                    "Type": f"Regex: {label}",
                    "Snippet": match.group(0).strip(),
                    "Offset": match.start()
                })
        return findings
    
    def _check_git_available(self) -> bool:
        """
        Check if git is available
        
        Returns:
            True if git is available, False otherwise
        """
        try:
            subprocess.run(
                ["git", "--version"],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def get_changed_files(self) -> List[str]:
        """
        Get list of files changed in git
        
        Returns:
            List of changed file paths
        """
        # Check if git is available
        if not self._check_git_available():
            self._log_warning("Git is not available. Defaulting to full scan.")
            return []
        
        for base in ["origin/main", "main", "HEAD~1"]:
            try:
                subprocess.run(
                    ["git", "rev-parse", "--verify", base],
                    check=True,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
                result = subprocess.run(
                    ["git", "diff", "--name-only", f"{base}...HEAD"],
                    capture_output=True,
                    text=True,
                    check=True
                )
                files = result.stdout.strip().splitlines()
                if not self.quiet:
                    print(f"ðŸ“‚ Scanning changes against: {base} ({len(files)} changed files)")
                return [f for f in files if os.path.isfile(f)]
            except subprocess.CalledProcessError:
                continue  # Try the next base
    
        self._log_warning("Could not determine a valid Git base for diff. Defaulting to full scan.")
        return []
    
    def get_files_to_scan(self) -> List[str]:
        """
        Get list of files to scan based on scan_path
        
        Returns:
            List of file paths to scan
        """
        current_file = os.path.abspath(__file__)
        files = []
        scan_path = self.scan_path
        
        # Handle scan_path - could be a file or directory
        if not os.path.exists(scan_path):
            self._log_error(f"Path does not exist: {scan_path}")
            return []
        
        # If scan_path is a file, scan just that file
        if os.path.isfile(scan_path):
            abs_path = os.path.abspath(scan_path)
            if abs_path != current_file and self.is_text_file(abs_path):
                files.append(abs_path)
            return files
        
        # If scan_path is a directory, proceed with directory scanning
        if not os.path.isdir(scan_path):
            self._log_error(f"Path is neither a file nor directory: {scan_path}")
            return []
        
        # Change to the scan directory for git operations
        original_dir = os.getcwd()
        try:
            os.chdir(scan_path)
            
            if self.full_scan_mode:
                # Scan all files in directory
                for root, dirs, filenames in os.walk("."):
                    # Modify dirs in-place to exclude certain directories
                    dirs[:] = [d for d in dirs if d not in self.config["excluded_dirs"]]
                    for name in filenames:
                        path = os.path.abspath(os.path.join(root, name))
                        if path != current_file and self.is_text_file(path):
                            files.append(path)
            else:
                # Scan only changed files (git mode)
                for rel_path in self.get_changed_files():
                    abs_path = os.path.abspath(rel_path)
                    if abs_path != current_file and self.is_text_file(abs_path):
                        files.append(abs_path)
        finally:
            os.chdir(original_dir)
        
        return files
    
    def process_file(self, file_path: str) -> Tuple[str, Optional[str], List[str]]:
        """
        Process a single file for PII detection
        
        Args:
            file_path: Path to the file
            
        Returns:
            Tuple of (file_path, error_message, findings)
        """
        findings = []
        try:
            # Check file size first - skip massive files
            file_size = os.path.getsize(file_path)
            max_size = self.config.get("max_file_size", 10_000_000)
            if file_size > max_size:
                return (file_path, f"âš ï¸ Skipping large file ({file_size/1_000_000:.1f}MB): {file_path}", [])
                
            with open(file_path, "r", encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            return (file_path, f"âš ï¸ Skipping due to read error: {e}", [])
    
        # Skip empty files
        if not content.strip():
            return (file_path, None, [])
            
        # Run regex scan on the whole content
        regex_hits = self.scan_with_regex(content)
        
        # Split content into manageable chunks for PII detection
        chunks = self.split_text_to_chunks(content)
        file_entities = []
        chunk_offset = 0
    
        for chunk in chunks:
            if not chunk.strip():  # skip empty chunks
                chunk_offset += len(chunk)
                continue
            entities = self.detect_pii(chunk, content, chunk_offset)
            filtered = [e for e in entities if e["Type"] in self.config["security_relevant_entity_types"]]
            file_entities.extend(filtered)
            chunk_offset += len(chunk)
    
        # Process findings
        if file_entities or regex_hits:
            findings.append(f"âš ï¸ Sensitive info detected in: {file_path}")
            
            # Group non-critical entities by type for consolidation
            entity_by_type = defaultdict(list)
            critical_entities = []
            
            for entity in file_entities:
                if entity["Type"] in self.config["critical_entity_types"]:
                    critical_entities.append(entity)
                else:
                    entity_by_type[entity["Type"]].append(entity)
            
            # Output critical entities with detail
            for entity in critical_entities:
                if "EntityText" in entity:
                    if "Context" in entity:
                        findings.append(f" - [PII] Type: {entity['Type']}, Score: {entity['Score']:.2f}, Value: \"{entity['EntityText']}\"")
                        
                        # Format multi-line context with vertical bars for each line
                        context_lines = entity['Context'].split('\n')
                        findings.append(f"   â”‚ Context:")
                        for line in context_lines:
                            findings.append(f"   â”‚    {line}")
                    else:
                        findings.append(f" - [PII] Type: {entity['Type']}, Score: {entity['Score']:.2f}, Value: \"{entity['EntityText']}\"")
                else:
                    findings.append(f" - [PII] Type: {entity['Type']}, Score: {entity['Score']:.2f}")
                    
            # Output consolidated non-critical entities
            for entity_type, entities in entity_by_type.items():
                if len(entities) > 3:  # Only consolidate if there are many
                    best_score = max(e["Score"] for e in entities)
                    findings.append(f" - [PII] Type: {entity_type}, Count: {len(entities)}, Best Score: {best_score:.2f}")
                else:
                    for entity in entities:
                        if "EntityText" in entity:
                            findings.append(f" - [PII] Type: {entity_type}, Score: {entity['Score']:.2f}, Value: \"{entity['EntityText']}\"")
                        else:
                            findings.append(f" - [PII] Type: {entity_type}, Score: {entity['Score']:.2f}")
                    
            # Add regex findings (always show in detail)
            for regex_hit in regex_hits:
                findings.append(f" - [Regex] {regex_hit['Type']} at offset {regex_hit['Offset']}: {regex_hit['Snippet']}")
                
            return (file_path, None, findings)
        else:
            return (file_path, None, [])
    
    def format_output(self, all_findings: List[List[str]]) -> str:
        """
        Format the findings according to the specified output format
        
        Args:
            all_findings: List of findings for each file
            
        Returns:
            Formatted output string
        """
        if self.output_format == "json":
            # Convert to JSON format
            json_output = []
            for file_findings in all_findings:
                if not file_findings:
                    continue
                    
                file_path = file_findings[0].split(": ", 1)[1]
                issues = []
                
                for line in file_findings[1:]:
                    if line.startswith(" - "):
                        issues.append(line[3:])
                
                json_output.append({
                    "file": file_path,
                    "issues": issues
                })
                
            return json.dumps(json_output, indent=2)
            
        elif self.output_format == "csv":
            # Convert to CSV format
            csv_lines = ["file,issue_type,details"]
            
            for file_findings in all_findings:
                if not file_findings:
                    continue
                    
                file_path = file_findings[0].split(": ", 1)[1]
                
                for line in file_findings[1:]:
                    if line.startswith(" - "):
                        parts = line[3:].split(": ", 1)
                        if len(parts) == 2:
                            issue_type = parts[0]
                            details = parts[1].replace('"', '""')  # Escape quotes for CSV
                            csv_lines.append(f'"{file_path}","{issue_type}","{details}"')
                        else:
                            csv_lines.append(f'"{file_path}","{line[3:]}",""')
            
            return "\n".join(csv_lines)
            
        else:  # Default to text format
            output_lines = []
            separator = "â”€" * 70  # Create a separator line
            
            for i, file_findings in enumerate(all_findings):
                # Add a separator before each file's findings
                output_lines.append(separator)
                
                for line in file_findings:
                    output_lines.append(line)
                
                # Add a separator after each file's findings
                output_lines.append(separator)
                
                # Add a newline between file findings (but not after the last one)
                if i < len(all_findings) - 1:
                    output_lines.append("")
            
            return "\n".join(output_lines)
    
    def scan(self) -> bool:
        """
        Run the PII scan
        
        Returns:
            True if issues were found, False otherwise
        """
        issues_found = False
        files_to_scan = self.get_files_to_scan()
        all_findings = []
    
        if not files_to_scan:
            if self.full_scan_mode:
                self._log_error("No files found during full scan. Something went wrong.")
                return True  # Return error status
            else:
                if not self.quiet:
                    print("ðŸ” No changed text files to scan (diff mode).")
                return False
    
        total_files = len(files_to_scan)
        if not self.quiet:
            print(f"ðŸ” Scanning {total_files} files for sensitive information...")
        
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config["max_workers"]) as executor:
            futures = {executor.submit(self.process_file, path): path for path in files_to_scan}
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                file_path, error_msg, findings = future.result()
                if error_msg:
                    if not self.quiet:
                        print(error_msg)
                        print()  # Add extra newline for spacing
                elif findings:
                    issues_found = True
                    all_findings.append(findings)
                
                # Periodically show progress
                if not self.quiet and (i+1) % 10 == 0 or (i+1) == total_files:
                    elapsed = time.time() - start_time
                    files_per_sec = (i+1) / elapsed if elapsed > 0 else 0
                    self._log_progress(f"Progress: {i+1}/{total_files} files processed ({files_per_sec:.1f} files/sec)", end="\r")
        
        # Print findings with proper spacing
        if all_findings:
            if not self.quiet:
                print("\n")  # Clear the progress line and add spacing
                
            # Format and print the output
            output = self.format_output(all_findings)
            print(output)
    
        if issues_found:
            if not self.quiet:
                print("\nâŒ Potential secrets/passwords found. Please remove or secure them.")
            return True
        else:
            if not self.quiet:
                print("\nâœ… No secrets or passwords detected.")
            return False


def parse_args() -> argparse.Namespace:
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Scan files for PII and sensitive information using AWS Comprehend",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument("path", nargs="?", default=".", help="File or directory to scan (default: current directory)")
    parser.add_argument("--all", action="store_true", help="Scan all files recursively (default: only scan git changes)")
    parser.add_argument("--config", type=str, help="Path to configuration file")
    parser.add_argument("--min-confidence", type=float, help="Minimum confidence score (0.0-1.0) for PII detection")
    parser.add_argument("--output", choices=["text", "json", "csv"], default="text", help="Output format")
    parser.add_argument("--custom-regex", type=str, help="Path to file containing custom regex patterns")
    parser.add_argument("--exclude-dirs", type=str, help="Comma-separated list of directories to exclude")
    parser.add_argument("--exclude-exts", type=str, help="Comma-separated list of file extensions to exclude")
    parser.add_argument("--workers", type=int, help="Number of worker threads")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose output")
    parser.add_argument("--quiet", "-q", action="store_true", help="Suppress all output except findings and errors")
    parser.add_argument("--region", type=str, help="AWS region to use for Comprehend API")
    
    return parser.parse_args()


def load_config(args: argparse.Namespace) -> Dict[str, Any]:
    """
    Load configuration from default config, config file, and command line arguments
    
    Args:
        args: Command line arguments
        
    Returns:
        Configuration dictionary
    """
    # Start with default config
    config = DEFAULT_CONFIG.copy()
    
    # Load from config file if provided
    if args.config:
        try:
            with open(args.config, 'r') as f:
                file_config = json.load(f)
                config.update(file_config)
        except Exception as e:
            print(f"Error loading config file: {e}", file=sys.stderr)
    
    # Override with command line arguments
    config["scan_path"] = args.path
    config["full_scan"] = args.all
    config["output_format"] = args.output
    config["verbose"] = args.verbose
    config["quiet"] = args.quiet
    config["custom_regex_file"] = args.custom_regex
    config["region"] = args.region
    
    if args.min_confidence is not None:
        config["min_confidence_score"] = args.min_confidence
        
    if args.workers is not None:
        config["max_workers"] = args.workers
        
    if args.exclude_dirs:
        additional_dirs = set(args.exclude_dirs.split(','))
        config["excluded_dirs"].update(additional_dirs)
        
    if args.exclude_exts:
        additional_exts = set(args.exclude_exts.split(','))
        config["excluded_extensions"].update(additional_exts)
    
    return config


def main() -> int:
    """Main entry point"""
    # Check if help is requested
    if "--help" in sys.argv or "-h" in sys.argv:
        print(__doc__)
        return 0
        
    args = parse_args()
    config = load_config(args)
    
    scanner = PIIScanner(config)
    issues_found = scanner.scan()
    
    return 1 if issues_found else 0


if __name__ == "__main__":
    sys.exit(main())
